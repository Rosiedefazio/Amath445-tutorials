{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at a two layer NN (3 coloumms). Input size is number of layer in first of 2 layers. (so if you have like a 6 layer NN there are many inputs depending on what layer you are looking at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BasicMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) #nonlinear activiation function, relu is a standard, more complicated ones like selu and gelu \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#above is one way, below is another way, below is useful for multiple layers \n",
    "\n",
    "class BasicMLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network with one hidden layer usibng nn.Sequential.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BasicMLP2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)   \n",
    "    \n",
    "\n",
    "class BasicMLP3(nn.Module):\n",
    "    \"\"\"\n",
    "     A simple feedforward neural network with multiple hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_hidden_layers):\n",
    "        super(BasicMLP3, self).__init__()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(inplace=True))\n",
    "        self.hidden_layers = self._make_hidden_layers(n_hidden_layers, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#creates a list of linear layers within NN, \n",
    "    def _make_hidden_layers(self, n_hidden_layers, hidden_size):\n",
    "        layers = []\n",
    "        for _ in range(n_hidden_layers):\n",
    "            layers += [nn.Linear(hidden_size, hidden_size), nn.ReLU(inplace=True)]\n",
    "        return nn.Sequential(*layers) #* is a python thinng, it unpacks the list into arguments \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\")\n",
    "X, y = df.drop(columns=[\"Survived\", \"Name\"]), df[\"Survived\"]\n",
    "\n",
    "numerical_cols = ['Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n",
    "categorical_cols = ['Sex', 'Pclass']\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.1, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/9, random_state=42)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we can define custom datasets by subclassing `torch.utils.data.Dataset` and implementing the `__len__` and `__getitem__` methods. In our case this might look like unnecessarily complicated, but it will be useful when we want to use more complex datasets such as image data. Moreover, we can use the `torch.utils.data.DataLoader` class to load the data in batches and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "  '''\n",
    "  Prepare the dTitanic dataset for classification.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X, y): #numpy array \n",
    "    if not torch.is_tensor(X) or not torch.is_tensor(y):\n",
    "      self.X = torch.from_numpy(X).float()\n",
    "      self.y = torch.from_numpy(y) #makes then torch arrays \n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return {\"X\": self.X[i], \"label\": self.y[i]} #using dictonary because he like dictonary \n",
    "  #could also do return self.X[i], self.y[i]\n",
    "  #can make a funstion so that no memoery is ussed, like when you are going through an image dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(y_true: np.ndarray, y_preds: np.ndarray):\n",
    "    conf_matrix = confusion_matrix(\n",
    "        y_true, y_preds, labels=[0, 1]\n",
    "    )  # TODO: Add generalised labels\n",
    "    accuracy, f1, precision, recall = (\n",
    "        accuracy_score(y_true, y_preds),\n",
    "        f1_score(y_true, y_preds, zero_division=0.0),\n",
    "        precision_score(y_true, y_preds, zero_division=0.0),\n",
    "        recall_score(y_true, y_preds, zero_division=0.0),\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }, conf_matrix\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def perform_inference(model, dataloader, device: str, loss_fn=None):\n",
    "    \"\"\"\n",
    "    Perform inference on given dataset using given model on the specified device. If loss_fn is provided, it also\n",
    "    computes the loss and returns [y_preds, y_true, losses].\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode, this disables training specific operations such as dropout and batch normalization\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "\n",
    "    print(\"[inference.py]: Running inference...\")\n",
    "    for i, batch in tqdm(enumerate(dataloader)):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        if loss_fn is not None:\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            y_true.append(labels.cpu().numpy())\n",
    "\n",
    "        preds = F.softmax(outputs.detach().cpu(), dim=1).argmax(dim=1)\n",
    "        y_preds.append(preds.numpy())\n",
    "\n",
    "    model.train()  # Set the model back to training mode\n",
    "    y_true, y_preds = np.concatenate(y_true), np.concatenate(y_preds)\n",
    "    return y_true, y_preds, np.mean(losses) if losses else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see more uses of Dataset and DataLoader in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TitanicDataset(X_train, y_train)\n",
    "valid_dataset = TitanicDataset(X_valid, y_valid)\n",
    "test_dataset = TitanicDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=4, shuffle=True) # Shuffle the training data on each epoch\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, num_workers=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, num_workers=4, shuffle=False)\n",
    "\n",
    "output_dir = \"./models/\"\n",
    "os.makedirs(os.path.join(output_dir, \"saved_models\"), exist_ok=True)\n",
    "n_epochs = 50\n",
    "batches_done = 0\n",
    "sbest_loss_val = float(\"inf\")\n",
    "epoch_metrics = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(y.unique())\n",
    "model = BasicMLP(X_train.shape[1], 128, num_classes).to(device)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "print(\"[train.py]: Starting Training...\")\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    y_preds = []\n",
    "    y_train = []\n",
    "    losses = []\n",
    "    for i, data in tqdm(enumerate(train_dataloader)):\n",
    "        inputs, labels = data[\"X\"].to(device), data[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = F.softmax(outputs, dim=1).argmax(dim=1)\n",
    "        y_preds.append(preds.cpu().numpy())\n",
    "        y_train.append(labels.cpu().numpy())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        batches_done += 1\n",
    "\n",
    "    loss_train = torch.tensor(losses).mean().item()\n",
    "    y_train, y_preds = np.concatenate(y_train), np.concatenate(y_preds)\n",
    "    train_metrics, train_conf_matrix = evaluation_metrics(y_train, y_preds)\n",
    "\n",
    "    y_val, y_preds_val, loss_val = perform_inference(model, valid_dataloader, device, loss_fn)\n",
    "    val_metrics, val_conf_matrix = evaluation_metrics(y_val, y_preds_val)\n",
    "\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    print(\n",
    "        f'[TRAINING METRICS] Loss: {loss_train} | Accuracy: {train_metrics[\"accuracy\"]} | '\n",
    "        f'F1: {train_metrics[\"f1_score\"]} | Precision: {train_metrics[f\"precision\"]} | Recall: {train_metrics[\"recall\"]}'\n",
    "    )\n",
    "    print(\n",
    "        f'[VALIDATION METRICS] Loss: {loss_val} | Accuracy: {val_metrics[\"accuracy\"]} | '\n",
    "        f'F1: {val_metrics[\"f1_score\"]} | Precision: {val_metrics[f\"precision\"]} | Recall: {val_metrics[\"recall\"]}'\n",
    "    )\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimiser_state_dict\": optimiser.state_dict(),\n",
    "        \"best_loss_val\": best_loss_val,\n",
    "    }\n",
    "    print(f\"[train.py]: Saving model at epoch {epoch}...\")\n",
    "    \n",
    "    # Save the model checkpoint if the validation loss is the best we've seen so far\n",
    "    if loss_val < best_loss_val:\n",
    "        best_loss_val = loss_val\n",
    "        print(f\"[train.ry]: Found new best model at epoch {epoch}. Saving model...\")\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, \"saved_models\", \"best_model.pt\"),\n",
    "        )\n",
    "\n",
    "    epoch_metrics[epoch] = {\n",
    "        \"train_loss\": loss_train,\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"valid_loss\": loss_val,\n",
    "        \"valid_metrics\": val_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_metrics_df = pd.DataFrame.from_dict(epoch_metrics, orient=\"index\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(data=epoch_metrics_df, x=epoch_metrics_df.index, y=\"train_loss\", label=\"Training Loss\")\n",
    "sns.lineplot(data=epoch_metrics_df, x=epoch_metrics_df.index, y=\"valid_loss\", label=\"Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training a bigger model (`BasicMLP3` with more hidden layers) and see if you can get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
