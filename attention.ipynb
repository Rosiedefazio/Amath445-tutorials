{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "somehow representention these extra freatures of words instead of just they exist in the dictonary. Look up Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal is to map input tokens to output tokens such that the output capture rich semantic structure \n",
    "simpliest thing you can go is take linear combination of the input vectors \n",
    "$y_n = a_1x_1 +...+ a_nx_n$  and the a's are attention coefficents \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if output pays more attention to certian inputs it should be at the cost of paying less attention to other words. why do we want this? we are working with the assumption that not every a is impotant to the conext of the word. We also do not want the attention coefficents to not be negative. this allows for the mutlpile different sentence that convey the same meaning but have differnt grammatical structures. this leads to  $0 \\leq a_{mn} \\leq 1$  and $\\sum_{m =1}^{N} a_{mn} =1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terminology used in ML community: \n",
    "\n",
    "$x_m$ = queries \n",
    "\n",
    "$y_m$ = values \n",
    "\n",
    "$x_m$ = keys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remeber the dot product can be \n",
    "\n",
    "$ x_m \\cdot x_n = ||x_m|| \\cdot ||x_n|| cos(\\theta)$\n",
    "\n",
    "\n",
    "soft max is \n",
    "\n",
    "$ a_{mn} = \\frac{exp(x_n^T x_m)}{\\sum_{i}x_n^T x_m }$\n",
    "\n",
    "y = softmax $(XX^T)X$\n",
    "\n",
    "Note that so far there are no learnable paramaters right now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will introduce learnable paramters \n",
    "\n",
    "Y = softmax $(XUU^TX)XU$\n",
    "\n",
    "$X \\to X^n = XU$\n",
    "\n",
    "note: $XUU^TX$ is symmentric and $(XUU^TX)^T = X U U^T X^T$ and this is not what we want.\n",
    "\n",
    "example: Every Chisel is a tool but every tool is not chisel. This highlight that symmetry is not inheriant becasue order matters. How do we fixe this? \n",
    "\n",
    "lets use 3 different matrices \n",
    "\n",
    "K = $X W^k$ \n",
    "\n",
    "Q = $X W^q$ \n",
    "\n",
    "V = $X W^v$ \n",
    "\n",
    "which gives \n",
    "\n",
    "Y = Softmax $(QK^T) V$ \n",
    "\n",
    "\n",
    "$W^k $ and $W^q$ are the same shap \n",
    "\n",
    "so $W^k, W^q = D x D_k $\n",
    "\n",
    "and $W^v = D x D_v $\n",
    "\n",
    "where D is dimension and dimension is always fixed \n",
    "\n",
    "We have made the assumption that dot product is the way to measure closeness, but it doesnt have to be dot production, RKS  is another kernal method that ties into functional analysisi (mesure theory) ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we have covered so far is for one attention head\n",
    "\n",
    "while the paper for attention is all you need came out in 2017, in 2014 a person in montreal discovered the basic steps for attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi- Attention blocks\n",
    "\n",
    "$H_h = Attention (Q^h, K^h, V^h) = softmax(\\frac{QK^T}{\\sqrt{D_k}}V)$\n",
    "\n",
    "H is heads  and \n",
    "\n",
    "Y = $ Concat(H_1,H_2,...,H_h) W^0$\n",
    "\n",
    "each attention block learns on its own differnent thing indepedently ( atleSt thats what we think it does) W is a learnable matrix that assigns attention to each of the heads \n",
    "\n",
    "size of one attention block is (N x HD_v) so dim(W) = HD_v x D so that Dim(Y) = NxD \n",
    "\n",
    "Y is linear, so far there is no nonlinearilty \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats all attention is, no nonlinearilty, no NNs just blocks, transforms are just attentions and NN combined \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\to Y(X) \\to MLP (Y(X))$\n",
    "and $ MLP \\iff NN$\n",
    "THIS IS ONE LAYER \n",
    "\n",
    "NN is wear you introduct non-linearilty. One intution is that with attention you have computed how the attentions is for each word and then the NN is how you process \n",
    "\n",
    "dimentions input = 64, NN = 256, output = 64. the times 4 helps alot \n",
    "\n",
    "GPT- ? has 96 layers, they use layer norm \n",
    "\n",
    "and residual NN $X \\to Y(X) \\to Layernorm(MLP (Y(X)) + Y(X))$ same architeture as 2017 paper \n",
    "\n",
    "look up 3 blueone browns video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
